---
authors: Edoardo Spadolini (edoardo.spadolini@gmail.com)
state: draft
---

# RFD 0076 - Downtime minimization

## Required Approvers
* Engineering: TBD
* Security: TBD
* Product: TBD

## What

This RFD describes the causes of downtime that can occur during the course of normal cluster operation, regular maintenance (upgrades of cluster components - mainly auth nodes and proxy nodes, scheduled CA rotations with long grace period for all CAs) and emergency maintenance (typically user CA rotation with little to no grace period), and discusses best practices and recommendations for Teleport operators, changes that can be made to Teleport itself to improve the availability of services within the constraints of the current design, and recommends potential future development steps to further improve things by completely sidestepping some problems in certain situations.

## Why

As Teleport becomes a fundamental part of how infrastructure and computing resources are accessed throughout an organization, it's important to know when and why certain parts of the cluster can be unavailable and for how long, and if it's possible to reduce or ideally completely negate this downtime by spending more or less effort and resources.

## Details

### Auth connectivity and upgrades

For various reasons, all Teleport components need a valid connection to an auth server to start up and work correctly: service startup in Teleport will wait until a connection to Auth is established, the SSH service needs to know if it's connecting to auth through a proxy to know if it's running in direct connection mode or reverse tunnel mode, `tsh` opens an auth client to know (among other things) if the cluster is running in FIPS mode, new sessions (of various kinds) can't be established because we need to create a session object in the backend, push audit log events and check for locks. If the connection to the auth drops, connections will be unaffected if connectivity is restored within a grace time (at least 5 minutes for locks, depending on the `locking_mode` config option; any session by a user with a nonzero `max_connections` has 2 minutes before the loss of the associated semaphore lease causes the session to be terminated) but other functionality might be affected - for instance, if a session resource expires before connectivity is restored, the session can't be joined by other parties anymore (the new `SessionTracker` mechanism creates resources with a one hour expiration time which should be plenty, but the old-style `session.Session` only has a 30 second TTL which is often not enough for the node to reconnect).

We're currently requiring that upgrades to the Auth components happen while a single instance of the Auth is running: in other words, even in a HA configuration of Teleport, we first have to shut down all Auth servers from the old version, then spin up a single instance of the new version, then (once it signals readiness, presumably) the other replicas can be started. The reason for this is that we reserve the right to run once-off data migrations or format changes in the backend when Auth starts, and that could make any currently running Auth very very confused. This process doesn't usually take very long when running containerized Auth replicas in Kubernetes or Docker, but doing the same with VMs can take quite a bit longer, and cluster functionality will still be impacted while proxies and agents reconnect; a rolling rollout is often times easier to configure and execute, and only affects a portion of the cluster at a time.

The Auth service doesn't support graceful shutdowns; every client must be capable of reconnecting and retrying. Because of this and the current restrictions on different Auth versions running at the same time, in-place upgrades (with SIGHUP) are already unsupported. In-process restarts as a result of CA rotations will be discussed later.

### Proxy connectivity and upgrades

Most Teleport agents (all but SSH and Desktop Access in direct connection mode) and all Teleport clients connect through a Proxy to provide or use services: the Proxy acts as a proxy (sic!) for all services, sometimes exclusively, and it includes web clients for SSH, App Access and Desktop Access. It can be sidestepped in rare cases, like when connecting to a leaf cluster's proxy (or agents) directly, or when using `ssh` from OpenSSH to connect to a Teleport SSH server in direct connection mode, but normally all data will flow through a Proxy. The Proxy allows for graceful shutdowns and graceful restarts: upon receiving a QUIT (or a HUP, which will also trigger an in-place upgrade) the listeners will close, reverse tunnel clients will get notified of the shutdown (more on this later), and then the Proxy will wait until the last user connection terminates - only when that happens, the Proxy will actually shut down.

For direct connection agents (and for in-process App Access agents, technically, but only in setups with a single Proxy) full functionality is (re)gained as soon as the Proxy becomes available (again); connections through a Proxy that's shutting down will continue working (including connections to an Auth) until the connection ends (because of a session close) or the Proxy is forcibly terminated after some external timeout (the typical situation with managed containers or VMs). There is currently no notification sent to the user of a service about the Proxy that's currently serving the connection shutting down, nor does it seem like there's a reasonable way of doing that with the current design.

In-process restarts as a result of CA rotations will be discussed later.

### Reverse tunnels in mesh mode

Agents in reverse tunnel mode are only accessible through their reverse tunnel connection through one of the Proxies. In agent mesh mode, a connection to an agent through a proxy is only possible if the agent has a reverse tunnel connection to that specific proxy. Agents identify the Proxies that they're connected to using UUIDs as a key, receiving informations about the existence of other proxies via the same reverse tunnel connection using an ad-hoc "discovery" protocol in the same connection; the discovery protocol also uses the UUIDs of the Proxies as identifiers. During the restart of an agent, the new instance is going to be unavailable to each Proxy until it has connected to its reverse tunnel service, but the old instance will still maintain connections as long as at least one user connection is open - this could be extended to have a grace period in which the old instance will keep running, giving time to the new instance to connect. Multiple instances with the same UUID connected to the same reverse tunnel server can coexist: connections are routed in order from most recent to least recent, but even in case of a reconnection, functionality shouldn't be impacted (as both instances should behave in the same way).

When a Proxy itself restarts, the new instance begins with zero reverse tunnels and will thus error out on any client connection attempt; the old one will send a "please reconnect" message (new in v10, still needs to be backported) to all the reverse tunnel agents, which will begin reconnecting while still keeping the connection alive for existing connections (and for connections that were in-flight at the moment that the restart began); this will eventually restore connectivity, but some potential downtime is almost inevitable in this case. The same concerns apply when spinning up a new proxy, which doesn't have the downside of replacing an existing working proxy, but shares the issue of lacking reverse tunnel availability as soon as it starts up. It's possible to sidestep this problem by configuring the proxy load balancer to only route user connections to the well-established proxies, while letting reverse tunnel connections through to all the proxies; this, combined with an artificial delay before shutdown, would allow a zero-downtime upgrade of Proxies. How to actually do this split-world load balancing is left as an exercise for the reader (in TLS routing mode, reverse tunnel connections include `teleport-reversetunnel` in the ALPN protos in their ClientHello, which allows them to be distinguished from user connections).

### Reverse tunnels in proxy peering mode

In proxy peering mode, reverse tunnel agents become fully available as soon as they are connected to at least one proxy that's reachable by other proxies (and they heartbeat as such). It should be easy to maintain availability across a restart of an agent (as long as we add some grace period during the shutdown of the previous instance). Restarting a proxy in-place has the same problems that in-place restarting causes when in mesh mode, and should be avoided in this case as well. Spinning up a new proxy in proxy peering mode incurs no downtime, as the newly added proxy will be able to serve user connections immediately, by connecting to the other proxies - shutting down a proxy ungracefully will cause a loss of connectivity for all agents that are connected to just that proxy, however.

Leaf clusters will always connect to their parent cluster in mesh mode, so the advantages of proxy peering mode are not relevant in that case.

### CA rotations

The current implementation of CA rotations involves restarting every component in the cluster after each phase (other than `init`); as such, connectivity loss for reverse tunnel agents is inevitable (requiring potentially more effort than a full shutdown and startup, as newly restarted agents might initially connect to proxies that haven't restarted yet), and the fact that all Auths are restarted means that at least some disruption is to be expected, even for agents in direct connection mode. As most of the best practices around maintaining high availability and low downtime involve rolling upgrades and not restarting Auth and Proxy in place, it's clear that the current implementation of CA rotations is problematic. It should be possible to swap out credentials without a full restart on a rotation, and that would make most of the availability concerns around CA rotations disappear.

In addition, reverse tunnel connections from leaf clusters are currently severed whenever any relevant CA is rotated on either end of the connection, ungracefully terminating all existing connections going through the tunnel at the time.

## Best practices

In a HA scenario where it's important to maintain availability in the face of unpredictable events and minimize downtime during maintenance, a few extra precautions should be taken beyond what's explained in the setup, scaling and upgrading guides.

The Auth service should be ran on its own or together with the SSH service (and the metrics service, for health checks); as it can't be upgraded in place anyway (because of the versioning restrictions) and it doesn't gracefully shut down, it should either be manually stopped, upgraded and started (through some out-of-band access) or it should run in some ephemeral machine that can be thrown away and replaced as needed. Multiple instances of the Auth service can and should run at the same time, using the same underlying HA backend (etcd, DynamoDB, Firestore); if one fails or becomes unavailable, the orchestrator and the load balancer should detect the situation, redirect incoming connections to the other instances and spin up a new one. Minor downtime until nodes reconnect is expected, both in the upgrade and in the failover cases - in the latter, only limited to the Teleport instances that were connected to the faulted Auth. For upgrades and migrations/restarts, it's recommended to have infrastructure in place to quickly spin up new Auth instances.

The Proxy service acts as both the ingress point to the cluster for client connections (in browser and `tsh`) and as the handler for reverse tunnel connections for all the reverse tunnel services (including their connection to Auth). Especially because of the latter, in-process restarts and upgrades should be avoided. Graceful shutdown is supported, the proxy will wait for user connections to terminate and the proxy peering server will wait a fixed amount of time before exiting to serve connections for agents that haven't yet reconnected to a different proxy (not yet implemented).

A rolling upgrade strategy will work with no expected downtime when the cluster is in proxy peering mode, but extra care is required when using the traditional mesh network mode: whenever a new Proxy is added, the load balancer should use the aforementioned "split" configuration to allow reverse tunnel connections to reach the newly added Proxy, while only forwarding any user connection to the old, established proxies for some amount of time (to be determined based on cluster size and network conditions by seeing how quickly the new proxy reaches the same amount of reverse tunnels as the other proxies). Once the new Proxy has reached the appropriate amount of reverse tunnels, it can be added to the upstream rotation for user connections - this is valid advice for both upgrading to a new version and starting up a new Proxy after a fault (in the latter case, only if at least one established Proxy is still running).

A Proxy that's not also doubling as Auth (a configuration that's discouraged for any HA setup) can't run additional services other than metrics and SSH - running the App service through the in-memory channel should be discouraged in production (as it only works when the cluster has a single Proxy anyway), and so should the legacy Kube service.

Any peripheral agent roles can be restarted/upgraded in place (this is often the only option for reverse tunnel SSH service on non-ephemeral machines), but it's probably easier to run non-SSH services in some ephemeral container anyway. For the SSH service, care should be taken to either use the Teleport-managed SIGHUP restart/upgrade or to _ungracefully_ stop and start Teleport instead of waiting for a graceful shutdown to end before starting up a new instance, as the graceful shutdown has no (internal) timeout, so an existing SSH session in a forgotten terminal is going to prevent any new connection; the SIGHUP restart/upgrade fires up the new instance and waits for it to be ready before beginning the shutdown of the old one, so it's not susceptible to this issue. Readiness is currently signaled as soon as a single reverse tunnel connection is established, but that's only sufficient when running in proxy peering mode - in mesh mode, connectivity is still going to be impacted until the agent has connected to all proxies. If only a single agent (or a handful of services) needs to connect the expected time for this is quite small (growing superlinearly with the amount of proxies, sadly), but if a mass restart is triggered on a large cluster, the bottleneck could actually shift to the proxies themselves, as they need to process all the inbound connections from nodes. The planned automatic upgrade system will only trigger upgrades on a few nodes at a time for this reason, and so should any bespoke upgrade system if maintaining availability is critical.

## Potential future steps

* Remove the old-style `session.Session`: already underway, will be completed in v11.
* Allow running different Auth builds from the same major version at the same time (potentially restricted to only two versions, upgrading from old to new): potentially good value, requires engineering care and prevents us from running migrations outside of major version upgrades.
* Properly deprecate in-place upgrades when `auth_service` is enabled and `auth_servers` is pointing to just localhost (auth and other services in the same process).
* Add a grace period on shutdown of reverse tunnel agents and servers: will delay shutdowns - potentially for nothing, if the shutdown is meant to be a shutdown rather than a restart and we're not in proxy peering mode, but we could extend the internal "shutdown" protocol to also carry this information, as we always know if we have spawned a new Teleport ourselves or not.
* Backport the reconnection advisory mechanism for reverse tunnels.
* Don't close the proxy peering listeners when shutting down rather than restarting.
* Deprecate in-place restarts for Proxies in proxy peering mode.
* Restartless CA rotations: requires engineering effort, but would simplify things around the initialization code of `TeleportProcess` quite a bit. Would require some input from security, to decide what to do about preexisting connections after old certs become untrusted.
* Don't close and restart the reverse tunnel connection from leaf clusters after CA rotations.
* Allow for multiple Auth replicas to be spun up at the same time, and have them all wait for some time instead of erroring out when failing to grab the migration lock.
* Write user-facing docs regarding best practices around rollouts and upgrades with regards to downtime.
* Add waiting for reverse tunnels before shutting down the old service for peripheral nodes - agent pool saturation plus a timeout?
* Add support for graceful shutdowns to kube, db and desktop access.
* Define scenarios that warrant specific test cases (and write tests for those scenarios).
